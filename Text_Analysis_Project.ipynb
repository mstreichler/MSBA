{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis - Project\n",
    "Scarping twitter data to determine if iPhone or Android users are switching phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df1 = pd.read_csv(\"tweets.csv\")\n",
    "df2 = pd.read_csv(\"tweet1.csv\")\n",
    "df3 = pd.read_csv(\"tweet2.csv\")\n",
    "df4 = pd.read_csv(\"tweet3.csv\")\n",
    "df5 = pd.read_csv(\"tweet4.csv\")\n",
    "df6 = pd.read_csv(\"tweet5.csv\")\n",
    "df7 = pd.read_csv(\"tweet6.csv\")\n",
    "df8 = pd.read_csv(\"tweet7.csv\")\n",
    "df9 = pd.read_csv(\"tweet8.csv\")\n",
    "df10 = pd.read_csv(\"tweet9.csv\")\n",
    "df11 = pd.read_csv(\"tweet10.csv\")\n",
    "df12 = pd.read_csv(\"tweet11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make all tweet commetns lowercase\n",
    "def lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "# Function to only change the word counts to be 1 per comments\n",
    "def count_fix(s):\n",
    "    if s > 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "# Function to sum the mention counts of each word\n",
    "def sum_words(s):\n",
    "    return sum(s)\n",
    "\n",
    "# Function to determine if it's a tweet or re-tweet\n",
    "def cut_rt(s):\n",
    "    if s[:2] == 'rt':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any duplicate tweets based on 'Tweet Id' after joining all dataframes\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df7, df8, df9, df10, df11, df12])\n",
    "df.drop_duplicates(inplace = True)\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df[\"Tweet Text\"] = df[\"Tweet Text\"].apply(lowercase)\n",
    "\n",
    "# df now only contains tweets coming from an iPhone or an Android\n",
    "mask = (df[\"Source\"] == \"Twitter for iPhone\") | (df[\"Source\"] == \"Twitter for Android\")\n",
    "df = df[mask]\n",
    "\n",
    "# Creating dataframes to separate what iPhone users and Android users are talking about\n",
    "# Used for lift of each phone against top 10 attributes\n",
    "iphone_mask = df[\"Source\"] == \"Twitter for iPhone\"\n",
    "android_mask = df[\"Source\"] == \"Twitter for Android\"\n",
    "df_iphone = df[iphone_mask]\n",
    "df_android = df[android_mask]\n",
    "\n",
    "# Creates dataframes for original tweets and retweets\n",
    "df[\"RT\"] = df[\"Tweet Text\"].map(cut_rt)\n",
    "no_rt_mask = (df[\"RT\"] == 0)\n",
    "rt_mask = (df[\"RT\"] == 1)\n",
    "df_no_rt = df[no_rt_mask].copy(deep=True)\n",
    "df_rt = df[rt_mask].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iphone         10497\n",
       "12             10402\n",
       "rt              8184\n",
       "https           4825\n",
       "pro             3607\n",
       "apple           3516\n",
       "appleevent      2005\n",
       "a14             1795\n",
       "bionic          1784\n",
       "introducing     1740\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iPhone\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "\n",
    "# transform the vectorized count\n",
    "Count_data = CountVec.fit_transform(df_iphone[\"Tweet Text\"])\n",
    " \n",
    "#create dataframe of the counts for each word\n",
    "cv_iphone = pd.DataFrame(Count_data.toarray(), columns = CountVec.get_feature_names())\n",
    "\n",
    "# changing the counts to just count words once per comment\n",
    "iphone_count = cv_iphone.applymap(count_fix)\n",
    "\n",
    "# summing the word frequencies\n",
    "iphone_freqs = iphone_count.apply(sum_words).sort_values(ascending = False)\n",
    "\n",
    "# storing frequency counts in csv file\n",
    "iphone_freqs.to_csv('iphone_frequencies.csv')\n",
    "iphone_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iphone        5552\n",
       "12            5472\n",
       "rt            5245\n",
       "https         2815\n",
       "apple         1753\n",
       "pro           1579\n",
       "appleevent     828\n",
       "a14            769\n",
       "bionic         760\n",
       "chip           733\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Android\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "\n",
    "# transform the vectorized count\n",
    "Count_data = CountVec.fit_transform(df_android[\"Tweet Text\"])\n",
    " \n",
    "#create dataframe of the counts for each word\n",
    "cv_android = pd.DataFrame(Count_data.toarray(), columns = CountVec.get_feature_names())\n",
    "\n",
    "# changing the counts to just count words once per comment\n",
    "android_count = cv_android.applymap(count_fix)\n",
    "\n",
    "# summing the word frequencies\n",
    "android_freqs = android_count.apply(sum_words).sort_values(ascending = False)\n",
    "\n",
    "# storing frequency counts in csv file\n",
    "android_freqs.to_csv('android_frequencies.csv')\n",
    "android_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We picked these top attributes that all iPhone and Android users are talking about\n",
    "top_atts = [\"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate lift function - iPhone\n",
    "def calc_lift_iphone(a, b):\n",
    "    total_size = len(df_iphone)\n",
    "    filter_a = df_iphone[df_iphone[\"Tweet Text\"].str.contains(a)]\n",
    "    num_a = len(filter_a)\n",
    "    num_b = len(df_iphone[df_iphone[\"Tweet Text\"].str.contains(b)])\n",
    "    num_a_b = len(filter_a['Tweet Text'][filter_a['Tweet Text'].str.contains(b)])\n",
    "    return total_size * float(num_a_b) / float(num_a * num_b)\n",
    "\n",
    "# calculate lift function - Android\n",
    "def calc_lift_android(a, b):\n",
    "    total_size = len(df_android)\n",
    "    filter_a = df_android[df_android[\"Tweet Text\"].str.contains(a)]\n",
    "    num_a = len(filter_a)\n",
    "    num_b = len(df_android[df_android[\"Tweet Text\"].str.contains(b)])\n",
    "    num_a_b = len(filter_a['Tweet Text'][filter_a['Tweet Text'].str.contains(b)])\n",
    "    return total_size * float(num_a_b) / float(num_a * num_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating lift between iPhone and Android\n",
    "def calc_lift(a, b):\n",
    "    total_size = len(df)\n",
    "    filter_a = df[df[\"Tweet Text\"].str.contains(a)]\n",
    "    num_a = len(filter_a)\n",
    "    num_b = len(df[df[\"Tweet Text\"].str.contains(b)])\n",
    "    num_a_b = len(filter_a['Tweet Text'][filter_a['Tweet Text'].str.contains(b)])\n",
    "    return total_size * float(num_a_b) / float(num_a * num_b)\n",
    "\n",
    "# Halfing functions\n",
    "def half_lift(df):\n",
    "    i = 0\n",
    "    while i < len(df.columns):\n",
    "        j = 0\n",
    "        while j < i + 1:\n",
    "            if i == j:\n",
    "                df[df.columns[j]][df.index[i]] = 0\n",
    "            else:\n",
    "                df[df.columns[j]][df.index[i]] = ' ' \n",
    "            j += 1\n",
    "        i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lift matrix of phones:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iphone</th>\n",
       "      <th>android</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "      <td>1.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>android</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        iphone android\n",
       "phone                 \n",
       "iphone       0   1.125\n",
       "android              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lift between iPhone and Android\n",
    "phones = [\"iphone\", \"android\"]\n",
    "\n",
    "# create phone matrix \n",
    "phone_matrix = pd.DataFrame(columns = phones)\n",
    "for p in phones:\n",
    "    phone_matrix = phone_matrix.append(pd.Series(0, index = phone_matrix.columns), ignore_index=True)\n",
    "phone_matrix['phone'] = phones\n",
    "phone_matrix = phone_matrix.set_index('phone')\n",
    "\n",
    "# calculate lift between phones\n",
    "import copy\n",
    "\n",
    "df = df.dropna(how='any')\n",
    "lift_matrix = copy.deepcopy(phone_matrix)\n",
    "\n",
    "for phone1, series in list(lift_matrix.iterrows()):\n",
    "    for phone2 in series.index:\n",
    "        lift_matrix[phone2].loc[phone1] = calc_lift(phone1, phone2)\n",
    "\n",
    "print('Lift matrix of phones:')\n",
    "dissimilarity = copy.deepcopy(lift_matrix)\n",
    "half_lift(lift_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phone-attribute matrix dataframe\n",
    "phone_type = [\"iphone\", \"android\"]\n",
    "phone_att = pd.DataFrame(columns = top_atts)\n",
    "\n",
    "for p in phone_type:\n",
    "    phone_att = phone_att.append(pd.Series(0, index = phone_att.columns), ignore_index = True)\n",
    "\n",
    "phone_att['phone'] = phone_type\n",
    "phone_att = phone_att.set_index('phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 attributes we chose to look at are:\n",
      "1. new\n",
      "2. 5g\n",
      "3. mini\n",
      "4. chip\n",
      "5. powerful\n",
      "6. camera\n",
      "7. design\n",
      "8. charger\n",
      "9. display\n",
      "10. retina\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new</th>\n",
       "      <th>5g</th>\n",
       "      <th>mini</th>\n",
       "      <th>chip</th>\n",
       "      <th>powerful</th>\n",
       "      <th>camera</th>\n",
       "      <th>design</th>\n",
       "      <th>charger</th>\n",
       "      <th>display</th>\n",
       "      <th>retina</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>iphone</td>\n",
       "      <td>1.02668</td>\n",
       "      <td>1.0321</td>\n",
       "      <td>1.02919</td>\n",
       "      <td>1.03864</td>\n",
       "      <td>1.03994</td>\n",
       "      <td>1.02874</td>\n",
       "      <td>1.03717</td>\n",
       "      <td>1.02894</td>\n",
       "      <td>1.03515</td>\n",
       "      <td>1.03994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>android</td>\n",
       "      <td>1.10911</td>\n",
       "      <td>1.12046</td>\n",
       "      <td>1.1133</td>\n",
       "      <td>1.13185</td>\n",
       "      <td>1.13339</td>\n",
       "      <td>1.10337</td>\n",
       "      <td>1.12006</td>\n",
       "      <td>1.11033</td>\n",
       "      <td>1.13339</td>\n",
       "      <td>1.13339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             new       5g     mini     chip powerful   camera   design  \\\n",
       "phone                                                                    \n",
       "iphone   1.02668   1.0321  1.02919  1.03864  1.03994  1.02874  1.03717   \n",
       "android  1.10911  1.12046   1.1133  1.13185  1.13339  1.10337  1.12006   \n",
       "\n",
       "         charger  display   retina  \n",
       "phone                               \n",
       "iphone   1.02894  1.03515  1.03994  \n",
       "android  1.11033  1.13339  1.13339  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lifts for phones and attributes\n",
    "for phone, series in list(phone_att.iterrows()):\n",
    "    for att in series.index:\n",
    "        if phone == \"iphone\":\n",
    "            phone_att[att].loc[phone] = calc_lift_iphone(phone, att)\n",
    "        if phone == \"android\":\n",
    "            phone_att[att].loc[phone] = calc_lift_android(\"iphone\", att)\n",
    "\n",
    "print('The Top 10 attributes we chose to look at are:')\n",
    "for att in range(len(top_atts)):\n",
    "    print('{}. '.format(att + 1) + top_atts[att])\n",
    "    \n",
    "print(\"\")\n",
    "phone_att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_list_all(attribute_list):\n",
    "    counter = 0\n",
    "    match_list = []\n",
    "    for i in tweets_all:\n",
    "        for att in attribute_list:\n",
    "            match = re.findall(att + '[?!.,]+', re.escape(str(i)))\n",
    "            counter += 1\n",
    "            \n",
    "            if match not in match_list and match != []:\n",
    "                match_list.append(match)\n",
    "                \n",
    "                for x in match_list:\n",
    "                    \n",
    "                    for y in x:\n",
    "                        attribute_list.append(y)\n",
    "                        \n",
    "    return set(attribute_list)\n",
    "\n",
    "def update_list_rt(attribute_list):\n",
    "    counter = 0\n",
    "    match_list = []\n",
    "    for i in tweets_rt:\n",
    "        for att in attribute_list:\n",
    "            match = re.findall(att + '[?!.,]+', re.escape(str(i)))\n",
    "            counter += 1\n",
    "            \n",
    "            if match not in match_list and match != []:\n",
    "                match_list.append(match)\n",
    "                \n",
    "                for x in match_list:\n",
    "                    \n",
    "                    for y in x:\n",
    "                        attribute_list.append(y)\n",
    "                        \n",
    "    return set(attribute_list)\n",
    "\n",
    "def update_list_no_rt(attribute_list):\n",
    "    counter = 0\n",
    "    match_list = []\n",
    "    for i in tweets_no_rt:\n",
    "        for att in attribute_list:\n",
    "            match = re.findall(att + '[?!.,]+', re.escape(str(i)))\n",
    "            counter += 1\n",
    "            \n",
    "            if match not in match_list and match != []:\n",
    "                match_list.append(match)\n",
    "                \n",
    "                for x in match_list:\n",
    "                    \n",
    "                    for y in x:\n",
    "                        attribute_list.append(y)\n",
    "                        \n",
    "    return set(attribute_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install if necessary\n",
    "#!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "lex = SentimentIntensityAnalyzer(lexicon_file=\"vader_lexicon.txt\",\n",
    "                                 emoji_lexicon=\"emoji_utf8_lexicon.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df[\"Tweet Text\"]\n",
    "tweets_rt = df_rt[\"Tweet Text\"]\n",
    "tweets_no_rt = df_no_rt[\"Tweet Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = lex.polarity_scores(sentence)\n",
    "    comp = score[\"compound\"]\n",
    "    return comp\n",
    "\n",
    "def att_to_dict(attribute_list):\n",
    "    att_dict = dict.fromkeys(attribute_list, 0)\n",
    "    return att_dict\n",
    "\n",
    "def add_to_dict(att, original_atts, att_dict):\n",
    "    if att in original_atts:\n",
    "        att_dict[att] += 1\n",
    "    return att_dict\n",
    "\n",
    "def a14(s):\n",
    "    if 'A14' in s:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call list function to find all attributes with punctuation\n",
    "att_a = update_list_all(top_atts)\n",
    "att_r = update_list_rt(top_atts)\n",
    "att_n = update_list_no_rt(top_atts)\n",
    "\n",
    "#What's this for?\n",
    "#print(tweets_all.map(a14).sum())\n",
    "#print(tweets_rt.map(a14).sum())\n",
    "#print(tweets_no_rt.map(a14).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "# call list function to find all attributes with punctuation\n",
    "original_atts = [\"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]\n",
    "\n",
    "score_list_all = []\n",
    "\n",
    "att_scores_df_a = pd.DataFrame(columns = original_atts + ['tweet_sent'])\n",
    "\n",
    "\n",
    "for phrase in tweets_all:\n",
    "    #create dictionary from original attribute list to keep track of composite scores for each attribute per review\n",
    "    att_scores = att_to_dict(original_atts)\n",
    "    att_scores['tweet_sent'] = 0\n",
    "    att_scores['tweet_sent'] = sentiment_analyzer_scores(phrase)\n",
    "    \n",
    "    score_a = 0\n",
    "    count_a = 0\n",
    "    att1_count_a = 0\n",
    "    att2_count_a = 0\n",
    "    att3_count_a = 0\n",
    "    att4_count_a = 0\n",
    "    att5_count_a = 0\n",
    "    att6_count_a = 0\n",
    "    att7_count_a = 0\n",
    "    att8_count_a = 0\n",
    "    att9_count_a = 0\n",
    "    att10_count_a = 0\n",
    "    \n",
    "    arr_a = str(phrase).replace(\"'\",'').split()\n",
    "    its_a = [iter(arr_a), iter(arr_a[1:]), iter(arr_a[2:]), iter(arr_a[3:]), iter(arr_a[4:]), iter(arr_a[5:]), iter(arr_a[6:]), iter(arr_a[7:]), iter(arr_a[8:]), iter(arr_a[9:])]\n",
    "    parse_a = list(zip(its_a[0], its_a[1], its_a[2], its_a[3], its_a[4], its_a[5], its_a[6], its_a[7], its_a[8], its_a[9]))\n",
    "    \n",
    "    for i in range(len(parse_a)):\n",
    "        if i == 0:\n",
    "            if parse_a[i][0] in att_a:\n",
    "                score_a += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                count_a += 1  \n",
    "                \n",
    "                if original_atts[0] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att1_count_a += 1\n",
    "                if original_atts[1] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att2_count_a += 1\n",
    "                if original_atts[2] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att3_count_a += 1\n",
    "                if original_atts[3] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att4_count_a += 1\n",
    "                if original_atts[4] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att5_count_a += 1\n",
    "                if original_atts[5] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att6_count_a += 1\n",
    "                if original_atts[6] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att7_count_a += 1\n",
    "                if original_atts[7] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att8_count_a += 1\n",
    "                if original_atts[8] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att9_count_a += 1\n",
    "                if original_atts[9] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att10_count_a += 1\n",
    "\n",
    "        if parse_a[i][2] in att_a:\n",
    "            score_a += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "            count_a += 1\n",
    "            \n",
    "            if original_atts[0] in parse_a[i][0]:\n",
    "                att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att1_count_a += 1\n",
    "            if original_atts[1] in parse_a[i][0]:\n",
    "                att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att2_count_a += 1\n",
    "            if original_atts[2] in parse_a[i][0]:\n",
    "                att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att3_count_a += 1\n",
    "            if original_atts[3] in parse_a[i][0]:\n",
    "                att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att4_count_a += 1\n",
    "            if original_atts[4] in parse_a[i][0]:\n",
    "                att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att5_count_a += 1\n",
    "            if original_atts[5] in parse_a[i][0]:\n",
    "                att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att6_count_a += 1\n",
    "            if original_atts[6] in parse_a[i][0]:\n",
    "                att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att7_count_a += 1\n",
    "            if original_atts[7] in parse_a[i][0]:\n",
    "                att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att8_count_a += 1\n",
    "            if original_atts[8] in parse_a[i][0]:\n",
    "                att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att9_count_a += 1\n",
    "            if original_atts[9] in parse_a[i][0]:\n",
    "                att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                att10_count_a += 1\n",
    "                \n",
    "        if i == len(parse_a)-1:\n",
    "            if parse_a[i][3] in att_a:\n",
    "                score_a += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                count_a += 1\n",
    "                \n",
    "                if original_atts[0] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att1_count_a += 1\n",
    "                if original_atts[1] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att2_count_a += 1\n",
    "                if original_atts[2] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att3_count_a += 1\n",
    "                if original_atts[3] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att4_count_a += 1\n",
    "                if original_atts[4] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att5_count_a += 1\n",
    "                if original_atts[5] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att6_count_a += 1\n",
    "                if original_atts[6] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att7_count_a += 1\n",
    "                if original_atts[7] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att8_count_a += 1\n",
    "                if original_atts[8] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att9_count_a += 1\n",
    "                if original_atts[9] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att10_count_a += 1\n",
    "                    \n",
    "            if parse_a[i][4] in att_a:\n",
    "                score_a += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                count_a += 1\n",
    "                \n",
    "                if original_atts[0] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att1_count_a += 1\n",
    "                if original_atts[1] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att2_count_a += 1\n",
    "                if original_atts[2] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att3_count_a += 1\n",
    "                if original_atts[3] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att4_count_a += 1\n",
    "                if original_atts[4] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att5_count_a += 1\n",
    "                if original_atts[5] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att6_count_a += 1\n",
    "                if original_atts[6] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att7_count_a += 1\n",
    "                if original_atts[7] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att8_count_a += 1\n",
    "                if original_atts[8] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att9_count_a += 1\n",
    "                if original_atts[9] in parse_a[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_a[i]))\n",
    "                    att10_count_a += 1\n",
    "    \n",
    "    if att1_count_a == 0:\n",
    "        att_scores[original_atts[0]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[0]] = att_scores[original_atts[0]]/att1_count_a\n",
    "    if att2_count_a == 0:\n",
    "        att_scores[original_atts[1]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[1]] = att_scores[original_atts[1]]/att2_count_a\n",
    "    if att3_count_a == 0:\n",
    "        att_scores[original_atts[2]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[2]] = att_scores[original_atts[2]]/att3_count_a\n",
    "    if att4_count_a == 0:\n",
    "        att_scores[original_atts[3]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[3]] = att_scores[original_atts[3]]/att4_count_a\n",
    "    if att5_count_a == 0:\n",
    "        att_scores[original_atts[4]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[4]] = att_scores[original_atts[4]]/att5_count_a\n",
    "    if att6_count_a == 0:\n",
    "        att_scores[original_atts[5]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[5]] = att_scores[original_atts[5]]/att6_count_a\n",
    "    if att7_count_a == 0:\n",
    "        att_scores[original_atts[6]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[6]] = att_scores[original_atts[6]]/att7_count_a\n",
    "    if att8_count_a == 0:\n",
    "        att_scores[original_atts[7]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[7]] = att_scores[original_atts[7]]/att8_count_a\n",
    "    if att9_count_a == 0:\n",
    "        att_scores[original_atts[8]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[8]] = att_scores[original_atts[8]]/att9_count_a\n",
    "    if att10_count_a == 0:\n",
    "        att_scores[original_atts[9]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[9]] = att_scores[original_atts[9]]/att10_count_a\n",
    "        \n",
    "    att_scores_df_a = att_scores_df_a.append(att_scores, ignore_index = True)\n",
    "    \n",
    "    if count_a == 0:\n",
    "        score_list_all.append(np.nan)\n",
    "        \n",
    "    else:\n",
    "        score_list_all.append(score_a/count_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cada9fe2bcdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Getting an error here --> parse_r[i][2] in att 'list index is out of range'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mparse_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matt_r\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mscore_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msentiment_analyzer_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mcount_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# All Re-Tweets\n",
    "# call list function to find all attributes with punctuation\n",
    "original_atts = [\"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]\n",
    "\n",
    "score_list_rt = []\n",
    "\n",
    "att_scores_df_r = pd.DataFrame(columns = original_atts + ['tweet_sent'])\n",
    "\n",
    "\n",
    "for phrase in tweets_rt:\n",
    "    #create dictionary from original attribute list to keep track of composite scores for each attribute per review\n",
    "    att_scores = att_to_dict(original_atts)\n",
    "    att_scores['tweet_sent'] = 0\n",
    "    att_scores['tweet_sent'] = sentiment_analyzer_scores(phrase)\n",
    "    \n",
    "    score_r = 0\n",
    "    count_r = 0\n",
    "    att1_count_r = 0\n",
    "    att2_count_r = 0\n",
    "    att3_count_r = 0\n",
    "    att4_count_r = 0\n",
    "    att5_count_r = 0\n",
    "    att6_count_r = 0\n",
    "    att7_count_r = 0\n",
    "    att8_count_r = 0\n",
    "    att9_count_r = 0\n",
    "    att10_count_r = 0\n",
    "    \n",
    "    arr_r = str(phrase).replace(\"'\",'').split()\n",
    "    its_r = [iter(arr_r), iter(arr_r[1:]), iter(arr_r[2:]), iter(arr_r[3:]), iter(arr_r[4:]), iter(arr_r[5:]), iter(arr_r[6:]), iter(arr_r[7:]), iter(arr_r[8:]), iter(arr_r[9:])]\n",
    "    parse_r = list(zip(arr_r[0], arr_r[1], arr_r[2], arr_r[3], arr_r[4], arr_r[5], arr_r[6], arr_r[7], arr_r[8], arr_r[9]))\n",
    "    \n",
    "    for i in range(len(arr_r)):\n",
    "        if i == 0:\n",
    "            if arr_r[i][0] in att_r:\n",
    "                score_r += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                count_r += 1  \n",
    "                \n",
    "                if original_atts[0] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att1_count_r += 1\n",
    "                if original_atts[1] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att2_count_r += 1\n",
    "                if original_atts[2] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att3_count_r += 1\n",
    "                if original_atts[3] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att4_count_r += 1\n",
    "                if original_atts[4] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att5_count_r += 1\n",
    "                if original_atts[5] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att6_count_r += 1\n",
    "                if original_atts[6] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att7_count_r += 1\n",
    "                if original_atts[7] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att8_count_r += 1\n",
    "                if original_atts[8] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att9_count_r += 1\n",
    "                if original_atts[9] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att10_count_r += 1\n",
    "                    \n",
    "# Getting an error here --> parse_r[i][2] in att 'list index is out of range'\n",
    "        if parse_r[i][2] in att_r:\n",
    "            score_r += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "            count_r += 1\n",
    "            \n",
    "            if original_atts[0] in parse_r[i][0]:\n",
    "                att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att1_count_r += 1\n",
    "            if original_atts[1] in parse_r[i][0]:\n",
    "                att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att2_count_r += 1\n",
    "            if original_atts[2] in parse_r[i][0]:\n",
    "                att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att3_count_r += 1\n",
    "            if original_atts[3] in parse_r[i][0]:\n",
    "                att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att4_count_r += 1\n",
    "            if original_atts[4] in parse_r[i][0]:\n",
    "                att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att5_count_r += 1\n",
    "            if original_atts[5] in parse_r[i][0]:\n",
    "                att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att6_count_r += 1\n",
    "            if original_atts[6] in parse_r[i][0]:\n",
    "                att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att7_count_r += 1\n",
    "            if original_atts[7] in parse_r[i][0]:\n",
    "                att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att8_count_r += 1\n",
    "            if original_atts[8] in parse_r[i][0]:\n",
    "                att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att9_count_r += 1\n",
    "            if original_atts[9] in parse_r[i][0]:\n",
    "                att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                att10_count_r += 1\n",
    "                \n",
    "        if i == len(parse_r)-1:\n",
    "            if parse_r[i][3] in att_r:\n",
    "                score_r += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                count_r += 1\n",
    "                \n",
    "                if original_atts[0] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att1_count_r += 1\n",
    "                if original_atts[1] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att2_count_r += 1\n",
    "                if original_atts[2] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att3_count_r += 1\n",
    "                if original_atts[3] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att4_count_r += 1\n",
    "                if original_atts[4] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att5_count_r += 1\n",
    "                if original_atts[5] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att6_count_r += 1\n",
    "                if original_atts[6] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att7_count_r += 1\n",
    "                if original_atts[7] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att8_count_r += 1\n",
    "                if original_atts[8] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att9_count_r += 1\n",
    "                if original_atts[9] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att10_count_r += 1\n",
    "                    \n",
    "            if parse_r[i][4] in att_r:\n",
    "                score_r += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                count_r += 1\n",
    "                \n",
    "                if original_atts[0] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att1_count_r += 1\n",
    "                if original_atts[1] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att2_count_r += 1\n",
    "                if original_atts[2] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att3_count_r += 1\n",
    "                if original_atts[3] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att4_count_r += 1\n",
    "                if original_atts[4] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att5_count_r += 1\n",
    "                if original_atts[5] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att6_count_r += 1\n",
    "                if original_atts[6] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att7_count_r += 1\n",
    "                if original_atts[7] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att8_count_r += 1\n",
    "                if original_atts[8] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att9_count_r += 1\n",
    "                if original_atts[9] in parse_r[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_r[i]))\n",
    "                    att10_count_r += 1\n",
    "    \n",
    "    if att1_count_r == 0:\n",
    "        att_scores[original_atts[0]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[0]] = att_scores[original_atts[0]]/att1_count_r\n",
    "    if att2_count_r == 0:\n",
    "        att_scores[original_atts[1]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[1]] = att_scores[original_atts[1]]/att2_count_r\n",
    "    if att3_count_r == 0:\n",
    "        att_scores[original_atts[2]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[2]] = att_scores[original_atts[2]]/att3_count_r\n",
    "    if att4_count_r == 0:\n",
    "        att_scores[original_atts[3]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[3]] = att_scores[original_atts[3]]/att4_count_r\n",
    "    if att5_count_r == 0:\n",
    "        att_scores[original_atts[4]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[4]] = att_scores[original_atts[4]]/att5_count_r\n",
    "    if att6_count_r == 0:\n",
    "        att_scores[original_atts[5]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[5]] = att_scores[original_atts[5]]/att6_count_r\n",
    "    if att7_count_r == 0:\n",
    "        att_scores[original_atts[6]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[6]] = att_scores[original_atts[6]]/att7_count_r\n",
    "    if att8_count_r == 0:\n",
    "        att_scores[original_atts[7]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[7]] = att_scores[original_atts[7]]/att8_count_r\n",
    "    if att9_count_r == 0:\n",
    "        att_scores[original_atts[8]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[8]] = att_scores[original_atts[8]]/att9_count_r\n",
    "    if att10_count_r == 0:\n",
    "        att_scores[original_atts[9]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[9]] = att_scores[original_atts[9]]/att10_count_r\n",
    "        \n",
    "    att_scores_df_r = att_scores_df_r.append(att_scores, ignore_index = True)\n",
    "    \n",
    "    if count_r == 0:\n",
    "        score_list_rt.append(np.nan)\n",
    "        \n",
    "    else:\n",
    "        score_list_rt.append(score_r/count_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Re-Tweets\n",
    "# call list function to find all attributes with punctuation\n",
    "original_atts = [\"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]\n",
    "\n",
    "score_list_no_rt = []\n",
    "\n",
    "att_scores_df_n = pd.DataFrame(columns = original_atts + ['tweet_sent'])\n",
    "\n",
    "\n",
    "for phrase in tweets_no_rt:\n",
    "    #create dictionary from original attribute list to keep track of composite scores for each attribute per review\n",
    "    att_scores = att_to_dict(original_atts)\n",
    "    att_scores['tweet_sent'] = 0\n",
    "    att_scores['tweet_sent'] = sentiment_analyzer_scores(phrase)\n",
    "    \n",
    "    score_n = 0\n",
    "    count_n = 0\n",
    "    att1_count_n = 0\n",
    "    att2_count_n = 0\n",
    "    att3_count_n = 0\n",
    "    att4_count_n = 0\n",
    "    att5_count_n = 0\n",
    "    att6_count_n = 0\n",
    "    att7_count_n = 0\n",
    "    att8_count_n = 0\n",
    "    att9_count_n = 0\n",
    "    att10_count_n = 0\n",
    "    \n",
    "    arr_n = str(phrase).replace(\"'\",'').split()\n",
    "    its_n = [iter(arr_n), iter(arr_n[1:]), iter(arr_n[2:]), iter(arr_n[3:]), iter(arr_n[4:]), iter(arr_n[5:]), iter(arr_n[6:]), iter(arr_n[7:]), iter(arr_n[8:]), iter(arr_n[9:])]\n",
    "    parse_n = list(zip(its_n[0], its_n[1], its_n[2], its_n[3], its_n[4], its_n[5], its_n[6], its_n[7], its_n[8], its_n[9]))\n",
    "    \n",
    "    for i in range(len(parse_n)):\n",
    "        if i == 0:\n",
    "            if parse_n[i][0] in att_n:\n",
    "                score_n += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                count_n += 1  \n",
    "                \n",
    "                if original_atts[0] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att1_count_n += 1\n",
    "                if original_atts[1] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att2_count_n += 1\n",
    "                if original_atts[2] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att3_count_n += 1\n",
    "                if original_atts[3] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att4_count_n += 1\n",
    "                if original_atts[4] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att5_count_n += 1\n",
    "                if original_atts[5] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att6_count_n += 1\n",
    "                if original_atts[6] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att7_count_n += 1\n",
    "                if original_atts[7] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att8_count_n += 1\n",
    "                if original_atts[8] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att9_count_n += 1\n",
    "                if original_atts[9] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att10_count_n += 1\n",
    "\n",
    "        if parse_n[i][2] in att_n:\n",
    "            score_n += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "            count_n += 1\n",
    "            \n",
    "            if original_atts[0] in parse_n[i][0]:\n",
    "                att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att1_count_n += 1\n",
    "            if original_atts[1] in parse_n[i][0]:\n",
    "                att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att2_count_n += 1\n",
    "            if original_atts[2] in parse_n[i][0]:\n",
    "                att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att3_count_n += 1\n",
    "            if original_atts[3] in parse_n[i][0]:\n",
    "                att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att4_count_n += 1\n",
    "            if original_atts[4] in parse_n[i][0]:\n",
    "                att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att5_count_n += 1\n",
    "            if original_atts[5] in parse_n[i][0]:\n",
    "                att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att6_count_n += 1\n",
    "            if original_atts[6] in parse_n[i][0]:\n",
    "                att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att7_count_n += 1\n",
    "            if original_atts[7] in parse_n[i][0]:\n",
    "                att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att8_count_n += 1\n",
    "            if original_atts[8] in parse_n[i][0]:\n",
    "                att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att9_count_n += 1\n",
    "            if original_atts[9] in parse_n[i][0]:\n",
    "                att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                att10_count_n += 1\n",
    "                \n",
    "        if i == len(parse_n)-1:\n",
    "            if parse_n[i][3] in att_n:\n",
    "                score_n += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                count_n += 1\n",
    "                \n",
    "                if original_atts[0] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att1_count_n += 1\n",
    "                if original_atts[1] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att2_count_n += 1\n",
    "                if original_atts[2] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att3_count_n += 1\n",
    "                if original_atts[3] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att4_count_n += 1\n",
    "                if original_atts[4] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att5_count_n += 1\n",
    "                if original_atts[5] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att6_count_n += 1\n",
    "                if original_atts[6] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att7_count_n += 1\n",
    "                if original_atts[7] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att8_count_n += 1\n",
    "                if original_atts[8] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att9_count_n += 1\n",
    "                if original_atts[9] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att10_count_n += 1\n",
    "                    \n",
    "            if parse_n[i][4] in att_n:\n",
    "                score_n += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                count_n += 1\n",
    "                \n",
    "                if original_atts[0] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[0]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att1_count_n += 1\n",
    "                if original_atts[1] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[1]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att2_count_n += 1\n",
    "                if original_atts[2] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[2]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att3_count_n += 1\n",
    "                if original_atts[3] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[3]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att4_count_n += 1\n",
    "                if original_atts[4] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[4]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att5_count_n += 1\n",
    "                if original_atts[5] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[5]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att6_count_n += 1\n",
    "                if original_atts[6] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[6]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att7_count_n += 1\n",
    "                if original_atts[7] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[7]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att8_count_n += 1\n",
    "                if original_atts[8] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[8]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att9_count_n += 1\n",
    "                if original_atts[9] in parse_n[i][0]:\n",
    "                    att_scores[original_atts[9]] += sentiment_analyzer_scores(' '.join(parse_n[i]))\n",
    "                    att10_count_n += 1\n",
    "    \n",
    "    if att1_count_n == 0:\n",
    "        att_scores[original_atts[0]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[0]] = att_scores[original_atts[0]]/att1_count_n\n",
    "    if att2_count_n == 0:\n",
    "        att_scores[original_atts[1]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[1]] = att_scores[original_atts[1]]/att2_count_n\n",
    "    if att3_count_n == 0:\n",
    "        att_scores[original_atts[2]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[2]] = att_scores[original_atts[2]]/att3_count_n\n",
    "    if att4_count_n == 0:\n",
    "        att_scores[original_atts[3]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[3]] = att_scores[original_atts[3]]/att4_count_n\n",
    "    if att5_count_n == 0:\n",
    "        att_scores[original_atts[4]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[4]] = att_scores[original_atts[4]]/att5_count_n\n",
    "    if att6_count_n == 0:\n",
    "        att_scores[original_atts[5]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[5]] = att_scores[original_atts[5]]/att6_count_n\n",
    "    if att7_count_n == 0:\n",
    "        att_scores[original_atts[6]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[6]] = att_scores[original_atts[6]]/att7_count_n\n",
    "    if att8_count_n == 0:\n",
    "        att_scores[original_atts[7]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[7]] = att_scores[original_atts[7]]/att8_count_n\n",
    "    if att9_count_n == 0:\n",
    "        att_scores[original_atts[8]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[8]] = att_scores[original_atts[8]]/att9_count_n\n",
    "    if att10_count_n == 0:\n",
    "        att_scores[original_atts[9]] = np.nan\n",
    "    else:\n",
    "        att_scores[original_atts[9]] = att_scores[original_atts[9]]/att10_count_n\n",
    "        \n",
    "    att_scores_df_n = att_scores_df_n.append(att_scores, ignore_index = True)\n",
    "    \n",
    "    if count_n == 0:\n",
    "        score_list_no_rt.append(np.nan)\n",
    "        \n",
    "    else:\n",
    "        score_list_no_rt.append(score_n/count_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "mask1 = (df['Source'] == 'Twitter for iPhone') | (df['Source'] == 'Twitter for Android')\n",
    "df_all = df[mask1].copy(deep=True)\n",
    "\n",
    "# turn overall_sent_score into Series to append to beer DF\n",
    "df_all['avg_att_sent'] = pd.Series(score_list_all)\n",
    "\n",
    "# merge df with att_scores_df\n",
    "phone_sent_all = df_all.merge(att_scores_df_a, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Re-Tweets\n",
    "mask2 = (df_rt['Source'] == 'Twitter for iPhone') | (df_rt['Source'] == 'Twitter for Android')\n",
    "df_rt = df_rt[mask2].copy(deep=True)\n",
    "\n",
    "# turn overall_sent_score into Series to append to beer DF\n",
    "df_rt['avg_att_sent'] = pd.Series(score_list_rt)\n",
    "\n",
    "# merge df with att_scores_df\n",
    "phone_sent_rt = df_rt.merge(att_scores_df_r, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Re-Tweets\n",
    "mask3 = (df_no_rt['Source'] == 'Twitter for iPhone') | (df_no_rt['Source'] == 'Twitter for Android')\n",
    "df_no_rt = df_no_rt[mask3].copy(deep=True)\n",
    "\n",
    "# turn overall_sent_score into Series to append to beer DF\n",
    "df_no_rt['avg_att_sent'] = pd.Series(score_list_no_rt)\n",
    "\n",
    "# merge df with att_scores_df\n",
    "phone_sent_no_rt = df_no_rt.merge(att_scores_df_n, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phone_sent.avg_att_sent.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_sent</th>\n",
       "      <th>avg_att_sent</th>\n",
       "      <th>new</th>\n",
       "      <th>5g</th>\n",
       "      <th>mini</th>\n",
       "      <th>chip</th>\n",
       "      <th>powerful</th>\n",
       "      <th>camera</th>\n",
       "      <th>design</th>\n",
       "      <th>charger</th>\n",
       "      <th>display</th>\n",
       "      <th>retina</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0.044663</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0.065792</td>\n",
       "      <td>-0.013207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_sent  avg_att_sent  new  5g  mini  chip  powerful  \\\n",
       "Source                                                                         \n",
       "Twitter for Android    0.044663      0.006271  NaN NaN   NaN   NaN       NaN   \n",
       "Twitter for iPhone     0.065792     -0.013207  NaN NaN   NaN   NaN       NaN   \n",
       "\n",
       "                     camera  design  charger  display  retina  \n",
       "Source                                                         \n",
       "Twitter for Android     NaN     NaN      NaN      NaN     NaN  \n",
       "Twitter for iPhone      NaN     NaN      NaN      NaN     NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_sent_all[[\"Source\", \"tweet_sent\", \"avg_att_sent\", \"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]].groupby(['Source']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_att_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [avg_att_sent]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_sent_rt[[\"Source\", \"tweet_sent\", \"avg_att_sent\", \"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]].groupby(['Source']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_sent</th>\n",
       "      <th>avg_att_sent</th>\n",
       "      <th>new</th>\n",
       "      <th>5g</th>\n",
       "      <th>mini</th>\n",
       "      <th>chip</th>\n",
       "      <th>powerful</th>\n",
       "      <th>camera</th>\n",
       "      <th>design</th>\n",
       "      <th>charger</th>\n",
       "      <th>display</th>\n",
       "      <th>retina</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0.142783</td>\n",
       "      <td>0.072038</td>\n",
       "      <td>0.031683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0.111211</td>\n",
       "      <td>0.056984</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_sent  avg_att_sent       new      5g  mini    chip  \\\n",
       "Source                                                                          \n",
       "Twitter for Android    0.142783      0.072038  0.031683     NaN   NaN     NaN   \n",
       "Twitter for iPhone     0.111211      0.056984  0.020011  0.5574   0.0  0.4588   \n",
       "\n",
       "                     powerful  camera  design  charger  display  retina  \n",
       "Source                                                                   \n",
       "Twitter for Android       NaN     NaN     NaN      NaN      NaN     NaN  \n",
       "Twitter for iPhone        NaN     NaN     NaN      NaN      0.0     0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_sent_no_rt[[\"Source\", \"tweet_sent\", \"avg_att_sent\", \"new\", \"5g\", \"mini\", \"chip\", \"powerful\", \"camera\", \"design\", \"charger\", \"display\", \"retina\"]].groupby(['Source']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
